Gradient Descent about Batch:
1. Batch Gradient Descent, BGD      (batch size == sample size) (stable)
2. Stochastic Gradient Descent, SGD (batch size == 1) (introduce stochastic noise, easier to run away local minimun)

> the selection of batch size needs match with hardware processor

Momentum method(inertia):  
1. initial parameter value $\theta_0$  
   
   initial momentum value $m_0$
   
2. calculate gradient value $g_0$
   
   update momentum value $m_1=\lambda m_0-\eta g_0$
   
   updata parameter value $\theta_1=\theta_0+m_1$
   
3. calculate gradient value $g_1$
   
   update momentum value $m_2=\lambda m_1-\eta g_1$
   
   updata parameter value $\theta_2=\theta_1+m_2$
   



Gradient Descent about Batch: 
Batch Gradient Descent, BGD        (batch size == sample size) (stable) 
Stochastic Gradient Descent, SGD (batch size == 1) (introduce stochastic noise, easier to run away local minimun)
Mini-batch Gradient Descent         (1 < batch size < sample size)
 the selection of batch size needs match with hardware processor
Momentum method(inertia):
initial parameter value 
θ
0
 
initial momentum value 
m
0
 
calculate gradient value 
g
0
 
update momentum value 
m
1
=
λ
m
0
−
η
g
0
 
updata parameter value 
θ
1
=
θ
0
+
m
1
 
calculate gradient value 
g
1
 
update momentum value 
m
2
=
λ
m
1
−
η
g
1
 
updata parameter value 
θ
2
=
θ
1
+
m
2
 
