Gradient Descent about Batch:
1. Batch Gradient Descent, BGD      (batch size == sample size) (stable)
2. Stochastic Gradient Descent, SGD (batch size == 1) (introduce stochastic noise, easier to run away local minimun)

> the selection of batch size needs match with hardware processor

Momentum method(inertia):  
1. initial parameter value $\theta_0$  
   
   initial momentum value $m_0$
   
2. calculate gradient value $g_0$
   
   update momentum value $m_1=\lambda m_0-\eta g_0$
   
   updata parameter value $\theta_1=\theta_0+m_1$
   
3. calculate gradient value $g_1$
   
   update momentum value $m_2=\lambda m_1-\eta g_1$
   
   updata parameter value $\theta_2=\theta_1+m_2$
   
