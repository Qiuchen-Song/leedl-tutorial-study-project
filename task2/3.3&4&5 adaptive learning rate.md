adaptive learning rate:

- AdaGrad(Adaptive Gradient), the larger gradient, the smaller learning rate.
  - $$\theta_{t+1}^{i}\gets \theta_{t}^{i}-\eta g_{t}^{i}$$
  - $$g_{t}^{i}=\left.\frac{\partial L}{\partial \theta ^{i}}\right|_ {\theta =\theta_{t}}$$
  - $$\theta_{t+1}^{i}\gets \theta_{t}^{i}-\frac{\eta }{\sigma_{t}^{i}}g_{t}^{i}$$
  - $$\theta_{1}^{i}\gets \theta_{0}^{i}-\frac{\eta }{\sigma_{0}^{i}}g_{0}^{i}$$
  - $$\sigma_{0}{i}=\sqrt{(g_0^i)^2}=\left | g_0^i \right |$$
  - $$\theta_{2}^{i}\gets \theta_{1}^{i}-\frac{\eta }{\sigma_{1}^{i}}g_{1}^{i}$$
  - $$\sigma_{0}{i}=\sqrt{\frac{1}{2} \left [ (g_0^i)^2+(g_1^i)^2 \right ]}
