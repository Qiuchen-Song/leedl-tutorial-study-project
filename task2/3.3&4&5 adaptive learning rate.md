# adaptive learning rate:

- AdaGrad(Adaptive Gradient), the larger gradient, the smaller learning rate.
  - $$\theta_{t+1}^{i}\gets \theta_{t}^{i}-\eta g_{t}^{i}$$
  - $$g_{t}^{i}=\left.\frac{\partial L}{\partial \theta ^{i}}\right|_ {\theta =\theta_{t}}$$
  - $$\theta_{t+1}^{i}\gets \theta_{t}^{i}-\frac{\eta }{\sigma_{t}^{i}}g_{t}^{i}$$
  - $$\theta_{1}^{i}\gets \theta_{0}^{i}-\frac{\eta }{\sigma_{0}^{i}}g_{0}^{i}$$
  - $$\sigma_{0}{i}=\sqrt{(g_0^i)^2}=\left | g_0^i \right |$$
  - $$\theta_{2}^{i}\gets \theta_{1}^{i}-\frac{\eta }{\sigma_{1}^{i}}g_{1}^{i}$$
  - $$\sigma_{1}{i}=\sqrt{\frac{1}{2} \left[(g_0^i)^2+(g_1^i)^2\right]}$$
  - $$\theta_{3}^{i} \leftarrow \theta_{2}^{i}-\frac{\eta}{\sigma_{2}^{i}} g_{2}^{i} \quad \sigma_{2}^{i}=\sqrt{\frac{1}{3}\left[\left(g_{0}^{i}\right)^{2}+\left(g_{1}^{i}\right)^{2}+\left(g_{2}^{i}\right)^{2}\right]}$$
  - $$\theta_{t+1}^{i} \leftarrow \theta_{t}^{i}-\frac{\eta}{\sigma_{t}^{i}} g_{t}^{i} \quad \sigma_{t}^{i}=\sqrt{\frac{1}{t+1}\sum_{i=0}^{t}(g_t^i)^2 }$$

- RMSProp(Root Mean Squared propagation)
  - $$\sigma_{0}{i}=\sqrt{(g_0^i)^2}=\left | g_0^i \right |$$
  - $$\theta_{2}^{i}\gets \theta_{1}^{i}-\frac{\eta }{\sigma_{1}^{i}}g_{1}^{i} \sigma_{1}{i}=\sqrt{\alpha(\sigma_0^i)^2+(1-\alpha)(g_1^i)^2}$$
  - $$\theta_{3}^{i} \leftarrow \theta_{2}^{i}-\frac{\eta}{\sigma_{2}^{i}} g_{2}^{i} \quad \sigma_{2}^{i}=\sqrt{\alpha(\sigma_1^i)^2+(1-\alpha)(g_2^i)^2}$$
  - $$\theta_{t+1}^{i} \leftarrow \theta_{t}^{i}-\frac{\eta}{\sigma_{t}^{i}} g_{t}^{i} \quad \sigma_{t}^{i}=\sqrt{\alpha(\sigma_{t-1}^i)^2+(1-\alpha)(g_t^i)^2}$$

- Adam(Adaptive moment estimation) (RMSprop + momentum)

# learning rate scheduling
- learning rate decay(learning rate annealing)
  - $$\theta_{t+1}^{i}\gets \theta_{t}^{i}-\frac{\eta_t }{\sigma_{t}^{i}}g_{t}^{i}$$
- preheating
