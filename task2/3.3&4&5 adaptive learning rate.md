adaptive learning rate:

- AdaGrad(Adaptive Gradient), the larger gradient, the smaller learning rate.
  - $$\theta_{t+1}^{i}\gets \theta_{t}^{i}-\eta g_{t}^{i}$$
  - 
  - $$\boldsymbol{g}_{t}^{i}=\left.\frac{\partial L}{\partial \boldsymbol{\theta}^{i}}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}_{t}}$$
  - 
  - $$\theta_{t+1}^{i}\gets \theta_{t}^{i}-\frac{\eta}{\sigma _{t}^{i}}g_{t}^{i}$$
  - 
  - $$\theta_{1}^{i}\gets \theta_{0}^{i}-\frac{\eta}{\sigma _{0}^{i}}g_{0}^{i}$$ 
