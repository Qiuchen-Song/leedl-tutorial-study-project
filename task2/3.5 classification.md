# softmax function
$$
y_{i}^{`}=\frac{\exp(y_i)}{\textstyle \sum_{j}\exp(y_i)}
$$
# classification loss
1. mean squared error

$$
e=\sum_{i}(y_{i}-y_{i}^{`})^{2}
$$

2. cross entropy (minimizing cross entropy == maximizing likelihood)

$$
e=-\sum_{i}y_{i}\ln y_{i}^{`}
$$
